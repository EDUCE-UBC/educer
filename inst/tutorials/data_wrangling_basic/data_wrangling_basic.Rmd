---
title: "Introduction to data wrangling"
author: "Michelle Kang"
date: "05/02/2020"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
description: This file contains the first of three data wrangling tutorials using the tidyverse package in R. Along with an introduction on downloading and loading packages, this tutorial introduces loading and visualising tabular data tables, and the filter, slice and select functions of tidyverse.  
---

```{r setup, include = FALSE}
# General learnr setup
library(learnr)
knitr::opts_chunk$set(echo = TRUE)
library(educer)
# Helper function to set path to images to "/images" etc.
setup_resources()

# Tutorial specific setup
library(readr)
library(tidyverse)

OTU_metadata_table <- combined

subset_dat <- slice(geochemicals, 710, 713, 715, 716, 709, 717, 718, 719)

x = 10 #for boolean exercise
restricted_columns <- select(OTU_metadata_table, OTU0001,
                             OTU0002, OTU0004, Depth)
summary_solution_1 <-
  geochemicals %>%
  select(Cruise, Date, Depth, CTD_O2) %>%
  filter(Cruise == 72 & Depth >= 0.05)

summary_solution_2 <-
  geochemicals %>% 
  filter(CTD_O2 > 0 | NO3 > 0) %>% 
  select(Cruise, Depth)
```



## Motivation

### Why use R for data processing?

Imagine you are looking at an environmental library of 10,000 plasmids, and you are asked to make arrow plots of only the plasmids that are less than `7500 bp`, originating only from bacteria or archea, and that have shown activity in your screen. It is more likely than not that you are used to using programs like Excel for general data processing. But how would you do this particular task in excel? You would have to apply conditional filters to multiple rows, then find a program online that accepts annotated plasmid maps as input and manually make each plot.

It would be a time-consuming process, and one you would have to repeat every time you had to complete this particular workflow. Thankfully, R provides packages for data management that make the filtering process a breeze, and the results of your filtering can be fed into a package that can sequentially generate the plots you want. If you had reason, you could even write an R script that will generate these plots for you in one click!

While the learning curve is somewhat steeper than programs like Excel, R is a highly modular language that allows for implementation of many different workflows (almost anything you can imagine)! And more importantly, you can write *general* scripts that can take predictable raw input from another source and process it automatically into whatever shape or form you would like.

### Structure of Data Wrangling Tutorials

We have developed three "data wrangling" modules for R. "Data wrangling," the process of taking raw data and transforming it into another, possibly more useful form, is a linchpin of R competency. By learning these basic techniques, you will open the door to creating beautiful figures using `ggplot2`, training machine learning models with `caret`, and so on.

We have split the contents into beginner, intermediate, and advanced modules.

#### Beginner

In this tutorial, we will demonstrate how to load data from your disk in from another data format (usually `.csv`) into R, how to manipulate that data table in R by subsetting it and doing some light data processing, and how to write your processed data to your disk.


#### Intermediate

In this tutorial, we will discuss how to join different datasets together using the numerous `*_join()` functions found in R. We will also discuss how to turn "wide-format" data into "long-format" data, and vice-versa.

#### Advanced

Here, we will introduce the `purrr` package, which allows you to perform more advanced processing on subsets of your dataset in parallel, and how to apply your own custom functions to columns in your data table.


## Learning Goals

- Load tabular data using `read_csv()` and save the data to your R environment.
- Introduce the use of logical operators and conditional statements in R for subsetting your data.
- Use the `filter()`, `slice()` and `select()` methods to conditionally subset your data.
- Use the `mutate()` function to create new variables in your dataset, using your existing variables.
- Writing your processed data to your disk.


## What is the Tidyverse?

The [tidyverse](https://www.tidyverse.org/) is a collection of R packages for data wrangling, analysis, and visualization.

The main advantages of using the tidyverse to read in data over base R are:

- Faster data processing
- Seamless integration with other tidyverse functions
- Automatic designation of data types
- Data storage in tibble as opposed to data frames
    - Tibbles are data frames with an additional layer of formatting that causes them to print nicely in the console and always return a tibble in functions
    
    
A popular package for data wrangling is *dplyr* in the tidyverse. This package is so good at what it does, and integrates so well with other popular tools like *ggplot2*, that it has rapidly become the de-facto standard.

dplyr code is very readable because all operations are based on using dplyr functions or *verbs* (select, filter, mutate...). 

Typical data wrangling tasks in dplyr:

- `select()` a subset of variables (columns)
- `slice()` out rows by their ordinal position in the tbl
- `filter()` out a subset of observations (rows)
- `rename()` variables
- `arrange()` the observations by sorting a variable in ascending or descending order
- `mutate()` all values of a variable (apply a transformation)
- `group_by()` a variable and `summarise` data by the grouped variable
- `*_join()` two data frames into a single data frame

Each verb works similarly:

- Input data frame in the first argument.
- Other arguments can refer to variables as if they were local objects
- Output is another data frame

Before working with our data, we first want to make a copy of the raw data so that we may revert to it quickly if we make any mistakes. This is best practices for data science in general.

```{r eval = F}
working_data <- raw_data
```

We will then repeatedly overwrite this object with the assignment operator (`<-`) as we further process it in R, as follows.

```{r eval = F}
working_data <- working_data + 7
working_data <- working_data / 3
```

<!-- ### Data description test -->

<!-- ``` {r child = system.file("resources/markdown", "data_description.Rmd", package = "educer")} -->
<!-- ``` -->

### Data description
The data used throughout this module were collected as part of an on-going oceanographic time series program in Saanich Inlet, a seasonally anoxic fjord on the East coast of Vancouver Island, British Columbia.

The data that you will use in R are 16S amplicon profiles of microbial communities at several depths in Saanich Inlet from one time point in this series (August 2012). These ~300 bp sequences were processed using [mothur](https://www.mothur.org/wiki/Main_Page) to yield 97% (approximately species-level) operational taxonomic units (OTUs). 

`combined` is a comma-delimited table of counts of four OTUs in each sample, normalized to 100,000 sequences per sample and the corresponding conditions of each sample (Depth, NO2, NO3 etc). 

For a brief introduction to these data, see Hallam SJ et al. 2017. Monitoring microbial responses to ocean deoxygenation in a model oxygen minimum zone. Sci Data 4: 170158 [doi:10.1038/sdata.2017.158](https://www.nature.com/articles/sdata2017158).

Click the button below to save a copy of this data set to your computer:

```{r echo = FALSE}
# Download button shiny app UI
fluidRow(
  column(12, align = "center", downloadButton("downloadData", "Download"))
)

```

```{r context = "server"}
# Download button shiny app server
output$downloadData <- downloadHandler(
  filename = "combined.csv",
  content = function(file) {
    write_csv(combined, file)
  }
)
```

## Reading and Writing Data to Disk

### Reading in a Dataset

First, ensure that you have downloaded the `combined.csv` file from the previous section, and you have saved it to your working directory. If you saved the file to another location, the data import function below will fail. To check your working directory, you can run the following.

```{r eval = F}
getwd()
```

We can load our Saanich data into R with `read_csv()` for comma separated files and specify the arguments that describe our data as follows.

- `col_names`: tells R that the first row is column names, not data

```{r eval = F}
raw_data <- read_csv(file = "combined.csv", col_names = TRUE)
```

<!-- Load in the dataset from educer -->

```{r include=FALSE}
raw_data <- combined
```

Now our data is formatted nicely into table form, and we can have a look at it with the `head()` function.

```{r}
head(raw_data)
```

The `head()` function prints the first six rows of your dataset, alongside your column names. The `<dbl>` printed below column names like `Cruise` is the data type of the column. In this case, it's a `<dbl>`, which is short for [https://en.wikipedia.org/wiki/Double-precision_floating-point_format](double-precision floating-point format), a particular way of holding numbers in memory. The details are beyond the scope of this tutorial, but just keep in mind it's a different data type than, say, a character string (`<chr>`). Anyway, we see that our import was successful.

<!-- ### Writing Data to Disk -->

<!-- Since we want to further manipulate our dataset after reading it in, we need to save it as a variable in R like we did previously with the `<-` operator. You can name the object whatever you like, though this module will assume the names used below. -->

<!-- ```{r eval = FALSE} -->
<!-- OTU_metadata_table <- read_csv(file = "combined.csv", col_names = TRUE) -->
<!-- ``` -->

<!-- ```{r saving-quiz, echo = FALSE} -->
<!-- quiz( -->
<!--   question("How do we make sure the datatable is saved to 'OTU_metadata_table' in our environment?", -->
<!--            answer("Entering `\"OTU_metadata_table\"` (with quotes) in the console displays the table."), -->
<!--            answer("Entering `OTU_metadata_table` (no quotes) in the console displays the table.", correct = TRUE), -->
<!--            answer("`OTU_metadata_table` shows up in the \"Global Environment\" box on the top right hand corner.", correct = TRUE) -->
<!--            ) -->
<!-- ) -->
<!-- ``` -->

### Writing Data to Disk

Although we have done no processing to our dataset, let's assume that we have, and that we want to save a processed dataset to disk. First, let's make a dummy processed dataset.

```{r}
processed_data <- raw_data
```

Then, to save it to disk, we have to use the `write_csv()` command.


## Data exploration

Let's explore the data that we've imported into R. The simplest way to view your imported data is to view it as a "tibble" like this. This view displays a subset of large data table.

```{r tibble, exercise = TRUE, exercise.lines = 5}
OTU_metadata_table
```

`glimpse()` is a function that, as its name suggests, allows us to get a "glimpse" of the contents of a data table. Running `glimpse()` on a data table outputs the number of rows (observations), columns (variables), and lists each column name along with its type and a portion of its contents. Let's run `glimpse()` with our OTU_metadata_table like this:

```{r glimpse, exercise = TRUE, exercise.lines = 5}
glimpse(OTU_metadata_table)
```
from this we see that our table has 7 rows and 10 columns. Each $ is followed by a column name, with information on the contents following each column name. `glimpse()` lists all columns of a table

### Exercise

```{r glimpse-quiz, echo = FALSE}
quiz(
  question("Which columns are in the OTU_metadata_table?",
           answer("OTU001", correct = TRUE),
           answer("Otu002"),
           answer("72"),
           answer("NO3", correct = TRUE),
           answer("NO3_Mean"),
           answer("Mean_N2O", correct = TRUE),
           answer("Depth", correct = TRUE)
           )
)
```

If we only want the dimensions of a dataframe or table, we can use the `dim()` function which prints the number of rows followed by the number of columns. Simple functions to query just the number of rows or columns in a data table are `nrow()` and `ncol()`.

```{r dim, exercise = TRUE, exercise.lines = 5}
#number of rows followed by number of columns
dim(OTU_metadata_table)
#number of rows
nrow(OTU_metadata_table)
#number of columns
ncol(OTU_metadata_table)
```

We can list the column names using `colnames()`. 

```{R colnames,exercise = TRUE, exercise.lines = 5}
colnames(OTU_metadata_table)
```


## `select()`

You can use the `select()` function to keep only a subset of variables (columns). Let's select the variables `OTU0001`, `OTU0002`, `OTU0004`, `Depth`.

```{R select-1, exercise = TRUE, exercise.lines = 5}
restricted_columns <- select(OTU_metadata_table, OTU0001, OTU0002, OTU0004, Depth)
```

To view our new `restricted_columns` variable, just type in the variable name and run the code like this:

```{R  select-2, exercise = TRUE, exercise.lines = 5}
restricted_columns
```

### Exercise
As an exercise, select for only the depth and geochemical columns (Depth, NO3, Mean_NO2, Mean_N2O, and Mean_NH4) in `OTU_metadata_table` and name the new table `metadata`:

```{r select-exercise, exercise = TRUE, exercise.lines = 5}

```

```{r select-exercise-hint-1}
select(OTU_metadata_table, <variable1>, <variable2>, <...>)
```

```{r select-exercise-hint-2}
select(OTU_metadata_table, Depth, NO3, Mean_NO2, Mean_N2O, Mean_NH4)
```
<!-- ### Exercise: `select()` -->

<!-- Select the Cruise, Date, Depth, PO4, and WS_NO3 variables of the `geochemicals` data set -->

<!-- ```{r select-exercise-1, exercise=TRUE, exercise.lines = 3} -->

<!-- dat <- select() -->

<!-- dat -->
<!-- ``` -->




## Booleans

Booleans are logical statements that are either `TRUE` or `FALSE` but can not be anything in between. As an example, run the code below:

```{r boolean-exercise, exercise = TRUE, exercise.lines = 5}
x <- 6
y <- "cat"

x < 3
y == "dog"
```

The equation `x < 3` is `FALSE` because x is set to 6 in the line above. As a simple exercise, manipulate the above code to make both equations `TRUE`. 

```{r boolean-exercise-hint-1}
#x <- A number less than 3
#y <- A string

x < 3
y == "dog"
```

```{r boolean-exercise-hint-2}
x <- 1
y <- "dog"

x < 3
y == "dog"
```

Note that in R, `==` is used in Boolean equations and using a single `=` will result in error. As you may have noticed above a single `=` is used to set a variable to a value. 

For quick reference, here are the most commonly used statements and operators.

R Operator | Meaning
---------- | ---------------
`==`       | equals
`< or >`   | less/greater than
`<= or >= `| less/greater than or equal to
`%in%`     | in
`is.na`    | is missing (`NA`)
`!`        | not (as in not equal to `!=`)
`&`        | and
`|`        | or


### Exercise

Write a boolean equation for "x is greater than 6 or less than 12", it should return `TRUE` after running.

```{r boolean-exercise-2, exercise = TRUE, exercise.lines = 5}

```

```{r boolean-exercise-2-hint-1}
# Consider how we represent the "greater/less than" 
# operator and the "OR" operator in R.

# "x is greater than n" can be represented in the following way.
x > n
```

```{r boolean-exercise-2-hint-2}
# "x is greater than n or less than m" can be represented in the following way.
x > n | x < m
```


### `filter()`

Conditional statements and logical operators are important when working with data in R. We will practice using different conditional statements and logical operators on the oxygen data in a subset of the `geochemicals` data set. You can use `filter()` to select specific rows based on a logical condition of a variable.


```{r}
subset_dat <- slice(geochemicals, 710, 713, 715, 716, 709, 717, 718, 719)
```

`variable == value` returns rows where the variable matches the value:

```{r equal-to, exercise = TRUE, exercise.lines = 5}
filter(subset_dat, CTD_O2 == 204.259)
```

`variable != value` returns rows where the variable does not match the value:

```{r not-equal-to, exercise = TRUE, exercise.lines = 5}
filter(subset_dat, CTD_O2 != 204.259)
```

`variable > value` returns rows where the variable is greater than the value:

```{r greater-than, exercise = TRUE, exercise.lines = 5}
filter(subset_dat, CTD_O2 > 204.259)
```

`variable %in% values` returns rows where the variable matches one of the given values.
Values are provided as a vector `c(value1, value2, ...)`:

```{r match-in, exercise = TRUE, exercise.lines = 5}
filter(subset_dat, CTD_O2 %in% c(40.745, 204.259))
```

`is.na(variable)` returns rows where the variable is `NA` (Not Available).

```{r is-na, exercise = TRUE, exercise.lines = 5}
filter(subset_dat, is.na(CTD_O2))
```

`!condition` returns rows where the condition is not fulfilled.

```{r opposite, exercise = TRUE, exercise.lines = 5}
filter(subset_dat, !is.na(CTD_O2))
```

We can look for a range of values by finding the rows where the value of the variable is <= 120 **AND** >= 20 bu using the logical operator `&`.

```{r and, exercise = TRUE, exercise.lines = 5}
filter(subset_dat, CTD_O2 <= 120 & CTD_O2 >= 20)
```

Lastly, we can use the logical OR (`|`) to find the rows where the value is <= 50 **OR** >= 150.

```{r or, exercise = TRUE, exercise.lines = 5}
filter(subset_dat, CTD_O2 <= 50 | CTD_O2 >= 150)
```

### Exercise

As an exercise, restrict for rows where the value for "depth" is less than or equal to 135m. 
```{r filter-exercise, exercise = TRUE, exercise.lines = 5}

```

```{r filter-exercise-hint-1}
# Recall the general syntax for filtering.
filter(dataset, (column, operator, quantity))
```

```{r filter-exercise-hint-2}
filter(subset_dat, depth <= 135)
```

## `slice()`

We can also only choose to work with specific rows in our data table using the `slice()` function. 

To select a subset of observations (rows) by their ordinal position, we use the `slice()` function.

```{r slice-1, exercise = TRUE, exercise.lines = 5}
slice(OTU_metadata_table, 1)
```

You can list multiple ordinal postions to select multiple observations at once.

```{r slice-2, exercise = TRUE, exercise.lines = 5}
slice(OTU_metadata_table, 1, 2, 3, 4, 5)
```

If you would like to to select a range of observations, give the starting and end position separated by a colon like so: `<start>:<end>`.

```{r slice-3, exercise = TRUE, exercise.lines = 5}
slice(OTU_metadata_table, 1:5)
```

```{r slice-quiz, echo = FALSE}
quiz(
  question("What is the value of OTU0003 in the 6th row of OTU_metadata_table?",
           answer("0"),
           answer("156"),
           answer("178", correct=TRUE),
           answer("72")
           )
)
```

### Exercise: `slice()` and `select()`
Using `slice()` and `select()`, determine:

A) what depth value occurs in the 20th row?
B) what methane value occurs in the 170th row?
    
```{r slice_exercise, exercise=TRUE, exercise.lines=5}
dat <- geochemicals
```

```{r slice_exercise-hint-1}
# Recall that slice() allows you to find a row
# and select() allows you to find a column
slice(dat, 20)
select(dat, depth)
```

```{r slice_exercise-hint-2}
slice(select(dat, Depth), 20)
select(slice(dat, 170), methane)
```
## Summary Exercise

The `geochemicals` dataset is included in the "educer" package. This dataframe contains time series observations on the water column chemistry. Learn more about the `geochemicals` dataset by running the following line in your R console. 

```{r dataset_exercise, exercise = TRUE, exercise.lines = 5}
?geochemicals
```

Using the geochemical data:

1. Select the Cruise, Date, Depth, and oxygen variables.
2. Filter the data to retain data on Cruise 72 where Depth is greater than or equal to 0.05 km.

Your resulting pdat object should be a [`r dim(summary_solution_1)`] data frame. The data has been loaded for you into the `dat` variable.

```{r summary-exercise, exercise = TRUE, exercise.lines = 5}
dat <- geochemicals
```

### Challenge exercise: `select()` and `filter()`

If you want more practice or have previous experience in R, try this more challenging exercise! Be sure to create a fresh `dat`.

3. Keep only the Cruise and Depth variables and the rows where oxygen OR nitrate is greater than zero.

Your resulting pdat object should be a [`r dim(summary_solution_2)`] data frame. *Hint:* Can you filter based on a variable that you previously removed by not selecting it?

```{r summary-exercise-2, exercise = TRUE, exercise.lines = 5}
dat <- geochemicals
```

## Additional resources

* [R cheatsheets](https://www.rstudio.com/resources/cheatsheets/) also available in RStudio under Help > Cheatsheets
* [Introduction to dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)
* [dplyr tutorial](https://rpubs.com/justmarkham/dplyr-tutorial)
* [dplyr video tutorial](https://www.r-bloggers.com/hands-on-dplyr-tutorial-for-faster-data-manipulation-in-r/)
