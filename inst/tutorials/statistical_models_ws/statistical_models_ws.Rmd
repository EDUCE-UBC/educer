---
title: "Statistical models in R"
author: >
  Gil J. B. Henriques and Andrew Li<br>
  Based on notes by Yue Liu and Kim Dill-McFarland.<br>
  A tutorial by [ECOSCOPE](http://ecoscope.ubc.ca/) at UBC.
date: "version `r format(Sys.time(), '%B %d, %Y')`"
description:  >
  Learn to perform statistical analyses with complex data sets.
  You will learn about linear regression, ANOVA, ANCOVA, 
  mixed effects models, and generalized linear models.
output: 
  learnr::tutorial:
      progressive: true
      allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include = FALSE}
# General learnr setup
library(learnr) # Interactive Tutorials for R
knitr::opts_chunk$set(echo = FALSE)
library(educer) # A Collection of Tutorials on Using R in Data Science
# Helper function to set path to images to "/images" etc.
setup_resources()

library(tidyverse) # Easily Install and Load the 'Tidyverse' 
library(broom) # Convert Statistical Objects into Tidy Tibbles
```

## Introduction

In this tutorial, we will learn about a number of common statistical models. They all employ a family of techniques called *regression analysis*, where we estimate the relationship between an *independent variable* (also called a *predictor*) and a *dependent variable* (also called a *response* or *outcome*). We will introduce linear regression, ANOVA, ANCOVA, and mixed effects models for continuous response data, logistic regression binary response data, as well as Poisson and Negative Binomial regression for count response data.

You will learn:

-   The different functions used to build a statistical model in R,
-   The assumptions behind the different models,
-   How the formula object in R is used to specify all the model terms,
-   How to interpret the main effects and interaction terms in a model.

This is an intermediate tutorial that assumes some prior experience with base R (see our "Introduction to R and RStudio" tutorial) and with the tidyverse package collection (see our "Introduction to the R tidyverse" tutorial).

```{r child = system.file("resources/markdown", "how_to_follow_tutorials.Rmd", package = "educer")}
```

### Data description

Unlike our other tutorials, "Statistical Models" utilizes several datasets in order to accurately demonstrate the use of statistical tests. These datasets are built into base R or into R packages. You will find more information on each of these datasets in its relevant section(s) in this tutorial.
 
### Loading packages

The [tidyverse](https://www.tidyverse.org/) is a collection of R packages. Packages expand R's capabilities by providing additional functions. In this tutorial we will use many functions from tidyverse packages such as dplyr (for data manipulation; pronounced *dee-plier*) and ggplot2 (for data visualization). You can learn more about the tidyverse in our "Introduction to the R tidyverse" tutorial.

When you work in a script that makes heavy use of functions from a particular package, we recommend loading it at the top of your script. You can use the code below to load all of the tidyverse packages:

```{r echo = TRUE, message = FALSE}
library(tidyverse) # Easily install and load the tidyverse packages
```

In this tutorial, we will also occasionally use functions from the following packages: gapminder, datasets, lme4, car, lsmeans, HSAUR. Because we will only make light use of each of these packages, there is no need to load all of the packages up-front at the top of our script. Instead of loading the entire package, we will access only the specific functions we need using the double colon operator, `::`. You will learn more about this operator later in the tutorial.

Keep in mind that to use packages in a script, these packages *must already be installed* on your computer (see the "Introduction to R and RStudio" tutorial for more details about installing packages and working with scripts). By installing the educer package, all of the packages we will use in this tutorial were automatically installed as well.

## Analysis of variance (ANOVA)

ANOVA is used when you have data with:

-   a quantitative response/dependent variable ($Y$) such as:
  -   height
  -   salary
  -   number of offspring
-   one or more categorical explanatory/independent variable(s) ($X$) such as:
  -   eye color
  -   sex
  -   genotype at a given locus

For example, you would use ANOVA to address questions like:

-   Does diet has an effect on weight gain?
  -   response variable: weight gain (e.g., kg)
  -   explanatory variable: type of diet (e.g., low vs. medium vs. high sugar)
-   Does the type sexual relationship practiced influence the fitness of male Red-winged Blackbirds?
  -   response variable: fitness of male bird (e.g., number of eggs laid)
  -   explanatory variable: sexual relationship (e.g., monagamy vs. polygamy)

### Key assumptions

-   Observations in the sample are independent from each other.
-   ANOVA is robust to the non-normality of sample data
-   Homoscedasticity â€” the variance of data in groups should be the same. However, if the ANOVA is balanced (groups have equal sample size) the model is robust to unequal variance.

### The gist of the math

When we run an ANOVA, we calculate a test statistic (*F*-statistic) that compares the **between** group variation with the **within** group variation:

$$F=MSB/MSW$$
where $MSB =$ Mean Square Between (i.e., the mean squared distance from each observation to the sample mean) and $MSW =$ Mean Square Within (i.e., the mean squared distance from each observation to the group mean). Essentially, if there is greater variation between the groups than within the groups we will get a large test statistic value (and correspondingly, a small *p*-value) and reject that null hypothesis ($H_0$: population means of all groups are equal).

If you want to delve into ANOVA in more depth, check out [this video tutorial](https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library).

### One-way ANOVA with two groups

Now let's run some ANOVAs on real data! We will start simple, by comparing the mean of two-groups. We will focus first on the case of a single explanatory variable (which is called the *one-way* ANOVA).

There are two perfectly acceptable statistical tests in R that we could apply to compare data between two groups. The first, which you may be very familiar with, is the *t*-test. The second is the ANOVA. Interestingly, the *t*-test is really a special case of ANOVA that can be used when only comparing only two groups. ANOVA is a more general test that we will later see can be used with more than two groups as well as more than one explanatory variable. 

#### Load and explore the data

The first experiment we are going to analyze was done to address the question of whether sexual activity effects the longevity of male fruit flies. To assess this, we are going to use a modified version of the fruitfly data from the [faraway R package](https://cran.r-project.org/web/packages/faraway/faraway.pdf).

Our hypotheses for this experiment are as follows:

Null hypothesis, $H_0$: Sexual activity has no effect on the population mean longevity of male fruit flies.
Alternative hypothesis, $H_A$: Sexual activity has an effect on population mean longevity of male fruit flies.

Let's now load the `fruitfly` dataset from the faraway package and assign it to a variable called `df_fly`. We haven't loaded this package, so we have to explicitly write the name of the package in front of the dataset's name (separated by a double colon operator: `faraway::fruitfly`), so that R knows where to find it. Run the code box below to load and inspect the first few rows of the data. You will have to replace the `<placeholder>` by the name of the variable you have created:

```{r load-faraway, exercise = TRUE}
df_fly <- faraway::fruitfly
head(<variable_name>)
```

```{r load-faraway-solution}
df_fly <- faraway::fruitfly
head(df_fly)
```

```{r setup-fly, include = FALSE}
df_fly <- faraway::fruitfly
```

The first two columns are continuous variables representing the fruit fly's thorax length and longevity. The final column, `activity`, is a categorical variable that encodes the fly's sexual activity. If the male was kept solitary (`isolated`) or with pregnant females (either `one` or `many`), he was not sexually active. On the other hand, the male may have been kept with virgin females, in which case he was sexually active (`low` = one virgin female; `high` = eight virgin females).

The next step is to create two-level categorical groups from the numerical variables. We will also subset the data set to equal group sizes (by randomly selecting twenty observations from each group) If you are unfamiliar with the functions below, see our "Introduction to the R tidyverse" tutorial. Use the code box below to create the modified data frame and examine its first few rows:
 
```{r fly-2-groups, exercise = TRUE}
fly_2_groups <- df_fly %>% 
	# Change "activity" to a 2-level variable
	mutate(activity = ifelse(activity %in% c("isolated", "one", "many"),
														"no", "yes")) %>%
	# Change size to a 2-level variable (for later)
	mutate(thorax = ifelse(thorax <= 0.8, "short", "long")) %>% 
	# Subset to equal group sizes
	group_by(activity, thorax) %>% 
	sample_n(20)

# Examine first few rows of the new data frame
head(<variable_name>)
``` 

```{r fly-2-groups-solution}
fly_2_groups <- df_fly %>% 
	# Change "activity" to a 2-level variable
	mutate(activity = ifelse(activity %in% c("isolated", "one", "many"),
														"no", "yes")) %>%
	# Change size to a 2-level variable (for later)
	mutate(thorax = ifelse(thorax <= 0.8, "short", "long")) %>% 
	# Subset to equal group sizes
	group_by(activity, thorax) %>% 
	sample_n(20)

# Examine first few rows of the new data frame
head(fly_2_groups)
``` 
 
```{r setup-groups, include = FALSE}
fly_2_groups <- df_fly %>% 
	# Change "activity" to a 2-level variable
	mutate(activity = ifelse(activity %in% c("isolated", "one", "many"),
														"no", "yes")) %>%
	# Change size to a 2-level variable (for later)
	mutate(thorax = ifelse(thorax <= 0.8, "short", "long")) %>% 
	# Subset to equal group sizes
	group_by(activity, thorax) %>% 
	sample_n(20)
```
 
To further explore our new data set, use the `dim()` function (to get the number of rows and columns) and the `summary()` function (to get a summary of the data). 

```{r fly-2-explore, exercise = TRUE, exercise.setup = "setup-groups"}
# get number of rows and columns
dim(<variable_name>)

# see summary of data
summary(<variable_name>)
``` 

Now, we should visualize the data to understand its basic properties, find patterns, and explore modeling strategies. Exploratory graphs are usually made really quickly. We simply want to get a better understanding of the data at this point.

Modify the code box below to explore the effect of the explanatory variable, `longevity` on the response variable, `activity`:

```{r fly-2-eda, exercise = TRUE, excercise.setup = "setup-eda"}
# create a box plot for exploratory data analysis
<variable_name> %>%
  ggplot(aes(x = <explanatory_column>, y = <response_column>)) +
  geom_boxplot() +
  labs(x = "Sexually Active", y = "Longevity (days)")
```

```{r fly-2-eda-solution}
fly_2_groups %>%
  ggplot(aes(x = longevity, y = activity)) +
  geom_boxplot() +
  labs(x = "Sexually Active", y = "Longevity (days)")
```

```{r setup-eda, include = FALSE}
fly_2_groups <- df_fly %>% 
	mutate(activity = ifelse(activity %in% c("isolated", "one", "many"),
														"no", "yes")) %>%
	mutate(thorax = ifelse(thorax <= 0.8, "short", "long")) %>% 
	group_by(activity, thorax) %>% 
	sample_n(20)
```

#### Formula notation in R
To perform an ANOVA in R, we need to understand R's formula notation, as this is the first argument we give to the ANOVA function (`aov()`). The formula notation stars with providing the response variable, then a special character, `~` which can be read as "as a function of", and then the explanatory/independent variable(s). Thus, the formula notation for this experiment is:
```
longevity ~ activity
```
The formula notation can get more complex, such as including additional explanatory/independent variables or interaction terms. We will introduce these as we attempt more complex analyses later on.

#### ANOVA in R
We want to know if sexual activity has an effect on population mean longevity of male fruit flies. 
The `aov()` function can be used to create a model that answers this question. 
As stated above, the first argument to this function is the formula. 
The second argument is the name of the data frame variable, in our case  `fly_2_groups`.

Create an ANOVA model object called `fly_2_groups_model`:

```{r two-group, exercise = TRUE}
# The response variable is on the left and explanatory variable on the right
fly_2_groups_model <- aov(<response_column> ~ <explanatory_column>,
                          data = <variable_name>)
```

```{r two-group-solution}
fly_2_groups_model <- aov(longevity ~ activity, data = fly_2_groups)
```

The `aov()` function returns a "model" object. You can examine the results of the model with the `summary()` function:

```{r two-group-sum, exercise = TRUE}
summary(<model_object>)
```

```{r two-group-sum-solution, excercise.setup = "setup-two-group-sum"}
summary(<fly_2_groups_model>)
```

```{r setup-two-group-sum, include = FALSE}
fly_2_groups_model <- aov(longevity ~ activity, data = fly_2_groups)
```

So, what does this output mean? The most important result in regards to rejecting or failing to reject our null hypothesis is the right-most number, the p-value (`Pr(>F)`). In this simple one-way ANOVA, we have a single p-value which has a very small value. Given that this is much smaller then the commonly used threshold for rejecting the null hypothesis, p < 0.05, we can reject our null hypothesis that sexual activity has no effect on the population mean longevity of male fruit flies, and accept the alternative hypothesis that sexual activity **does** have an effect on population mean longevity of male fruit flies. 

#### **Exercise 1: 1-way ANOVA**
![](images/rstudio_logo.png){width=0.4in} Exercise.

Using ANOVA, test if fruit fly longevity is affected by size (as measured by thorax length).

  * What are your null and alternate hypotheses?
  * What can you conclude from these results?

Create a model variable called `exercise1` and use the `summary()` function to examine the output.

**Note: Hints will be provided for exercises but no solution. Try to figure it out!**
```{r ex1, exercise = TRUE}
<model_object> <- aov(<response_column> ~ <explanatory_column>,
                      data = <variable_name>)
<function_for_examining_output>(<model_object>)
```

```{r ex1-hint-1}
# The response column is thorax and the exploratory column is thorax
```

```{r ex1-hint-2}
# Name the model object exercise1!
```

```{r ex1-hint-3}
# To examine the output of your model, use the summary() function!
```

### One-way ANOVA with more than two groups 

As mentioned at the start of this section, an ANOVA can be used when there are more then 2 levels in your categorical explanatory/independent variable (unlike a t-test). For example, we will consider the following case: 

We are still interested in whether sexual activity affects the longevity of male fruit flies but we want to understand this at a finer level (i.e., does the amount of sexual activity matter?). Thus, in this experiment, there are 3 categories for sexual`activity`. Specifically, males were kept:

1. `none` - alone
2. `low` - with a new virgin fruit fly everyday
3. `high` - with a new set of virgin fruit flies everyday

So for this case, our hypotheses are as follows:

Null hypothesis, $H_0$: Mean sexual activity does not vary from group to group. $\mu_{isolated} = \mu_{low} = \mu_{high}$ 
Alternative hypothesis, $H_A$: At least one group's mean sexual activity differs from that of the other groups.

#### Explore and analyse the data

Using the same original fruit fly data, now we create our 3-level activity group and call it `fly_3_groups`:

```{r fly-3-groups, exercise = TRUE}
fly_3_groups <- df_fly %>% 
  # convert factors to character variables
  mutate_if(is.factor, as.character) %>%
	# Change "activity" to a 3-level variable
	mutate(activity = ifelse(activity %in% c("isolated", "one", "many"),
														"none", activity)) %>%
	# Subset to equal group sizes
	group_by(activity) %>% 
	sample_n(25)

head(<variable_name>)
``` 

```{r fly-3-groups-solution}
fly_3_groups <- df_fly %>% 
  # convert factors to character variables
  mutate_if(is.factor, as.character) %>%
	# Change "activity" to a 3-level variable
	mutate(activity = ifelse(activity %in% c("isolated", "one", "many"),
														"none", activity)) %>%
	# Subset to equal group sizes
	group_by(activity) %>% 
	sample_n(25)

head(fly_3_groups)
``` 
 
```{r setup-3groups, include = FALSE}
fly_3_groups <- df_fly %>% 
	# Change "activity" to a 3-level variable
	mutate(activity = ifelse(activity %in% c("isolated", "one", "many"),
														"none", activity)) %>%
	# Subset to equal group sizes
	group_by(activity) %>% 
	sample_n(25)
```

Now, let's **explore** these data again using the `dim` and `summary` function:

```{r fly-3-explore, exercise = TRUE, exercise.setup = "setup-groups"}
# get number of rows and columns
dim(<variable_name>)
# view first 6 rows of data
head(<variable_name>)
``` 

```{r fly-3-explore-solution}
# get number of rows and columns
dim(fly_3_groups)
# view first 6 rows of data
head(fly_3_groups)
``` 

Again, it is good practice to **visualize** the the data before we perform our statistical analysis. Modify the code below to make a boxplot showing the relationship between the explanatory variable, `activity`, and the response variable, `longevity`:

```{r fly-3-eda, exercise = TRUE, excercise.setup = "setup-eda"}
# Create a box plot for exploratory data analysis
<variable_name> %>%
  ggplot(aes(x = <explanatory_column>, y = <response_column>)) +
  geom_boxplot() +
  labs(x = "Sexually Active", y = "Longevity (days)")
```

```{r fly-3-eda-solution}
# Create a box plot for exploratory data analysis
fly_3_groups %>%
  ggplot(aes(x = activity, y = longevity)) +
  geom_boxplot() +
  labs(x = "Sexually Active", y = "Longevity (days)")
```

```{r setup-3-eda, include = FALSE}
fly_3_groups <- df_fly %>% 
  mutate_if(is.factor, as.character) %>%
	mutate(activity = ifelse(activity %in% c("isolated", "one", "many"),
														"none", activity)) %>%
	group_by(activity) %>% 
	sample_n(25)
```

So, it looks like the longevity for both low and high activity is lower than the longevity of the isolated male fruit fly. Can we conclude that there are differences in the true population means between any of these groups? Again, we turn to ANOVA to answer this.

Create an ANOVA model and call it `fly_3_model`.

```{r three-group, exercise = TRUE}
<model_object> <- aov(<response_column> ~ <explanatory_column>,
                      data = fly_3_groups)
```

```{r three-group-solution}
fly_3_model <- aov(longevity ~ activity, data = fly_3_groups)
```

Now, use the `summary()` function to examine the output of `fly_3_model` model object:

```{r three-group-sum, exercise = TRUE}
<function_for_examining_output>(<model_object>)
```

```{r three-group-sum-solution, excercise.setup = "setup-three-group-sum"}
summary(fly_3__model)
```

```{r setup-three-group-sum, include = FALSE}
fly_3_model <- aov(longevity ~ activity, data = fly_3_groups)
```

So what does this output mean? Similar to a two-group ANOVA, we look at the p-value to determine if we reject (or fail to reject) our null hypothesis. In this case, we have a single p-value which has a very small value, much smaller that the commonly used threshold for rejecting the null hypothesis, p <0.05. We can reject our null hypothesis that the population means for longevity of male fruit flies are equal across all groups, and accept the alternative hypothesis that **at least** one group's population mean differs from that of the other groups. 

But which one(s) differ? 

#### Assess which groups differ

This is something that ANOVA cannot tell us. To answer this, we need to either perform pairwise t-tests (followed by an adjustment or correction for multiple comparisons, such as a Bonferroni correction) **or** perform a contrast test (such as Tukey's honestly significant difference test, HSD). We'll do both here and show that we get similar results:

Modify the cell code below to perform a Bonferroni-corrected pairwise t-test to observe group differences:

```{r fly-3-pairs, exercise = TRUE}
pairwise.t.test(fly_3_groups$<response_column>,
                fly_3_groups$<explanatory_activity>,
                p.adjust.method = "bonferroni")
``` 
 
Run the code below to perform Tukey's HSD test to observe group differences:

```{r fly-3-tuckey, exercise = TRUE}
TukeyHSD(<model_object>)
``` 

From both of these tests, we see that there is no significant difference between the population mean longevity of male fruit flies who had no or little sexual activity. However, high sexual activity does appear to matter, since the population mean longevity of male fruit flies who had high sexual activity is significantly different from that of male flies who had either no or low sexual activity.

### Two-way ANOVA with two groups
Let's continue to add complexity to our ANOVA model. In this experiment, we are not only interested in how sexual activity might affect longevity; we are also interested in body size (assessed via thorax length). We do this because the literature indicates that body size affects fruit fly longevity. Thus we now have two categorical/explanatory variables to look at sexual activity. 

- activity: `no` and `yes`
- thorax length `short` and `long`

For this experiment, we have two sets of null and alternative hypotheses: 

**Hypotheses for sexual activity**

**Null Hypothesis, $H_{0}$**: $\mu_{No} = \mu_{Yes}$  
**Alternative Hypothesis, $H_{A}$**: $\mu_{No} \ne \mu_{Yes}$

**Hypotheses for thorax length**

**Null Hypothesis, $H_{0}$**: $\mu_{short} = \mu_{long}$  
**Alternative Hypothesis, $H_{A}$**: $\mu_{short} \ne \mu_{long}$

Now that we have our case setup, let's re-look at our two-level data but now notice the thorax information.

Fill in the blank code box below. Use the `dim()` and `head()` function to explore the `fly_2_groups` data:

```{r two-way_group, exercise = TRUE}

```

```{r two-way_group-solution, excercise.setup = "setup-two-group-sum"}
# Explore the data, this time pay attention to the thorax column as well
dim(fly_2_groups)
head(fly_2_groups)
```

Now, **visualize** the data set. Recall that `activity` and `thorax` are the explanatory variables, whereas `longevity` is the response variable. We will represent the two thorax length groups (long and short) with different colors:

```{r two-way_group_viz, exercise = TRUE}
fly_2_groups %>%
  ggplot(aes(x = <first_explanatory_column>, y = <response_column>, color = <second_explanatory_column>)) +
  geom_boxplot() +
  labs(x = "Sexual Activity",
       y = "Longevity (days)")
```

```{r two-way_group_viz-solution, excercise.setup = "setup-two-group-sum"}
fly_2_groups %>%
  ggplot(aes(x = activity, y = longevity, color = thorax)) +
  geom_boxplot() +
  labs(x = "Sexual Activity",
       y = "Longevity (days)")
```

This data visualization suggests that both sexual activity and thorax length may affect longevity. Let's confirm (or disprove) this intuition by performing a two-way (or two-factor) ANOVA.

To perform a two-way ANOVA, we modify the formula notation that we pass into `aov()` function by adding an additional explanatory variable through the use of the `+` sign and the name of the new variable. Thus, our new formula for this case is: 

```
longevity ~ activity + thorax 
```

Create a new model object called `fly_2_vars_model` using the `fly_2_groups` data. Then use the `summary()` function to view the output of your ANOVA model: 

```{r two-way_aov, exercise = TRUE}
# create an ANOVA "model" object
fly_2_vars_model <- aov(<response_variable> ~ <first_explanatory_variable> + <second_explanatory_variable>, data = <data_frame_name>)

# examine output of aov()
summary(<model_object>)
```

```{r two-way_aov-solution, excercise.setup = "setup-two-group-sum"}
fly_2_vars_model <- aov(longevity ~ activity + thorax, data = fly_2_groups)

summary(fly_2_vars_model)
```

Now, we see that we get back an additional line in our results summary. The new line corresponds to the effect of thorax length on longevity. The p-values for both  sexual activity and size are very, very small. Thus we can reject both of our null hypotheses and infer that both sexual activity and size have statistically significant effect on longevity.

### Two-way ANOVA with two groups including an interaction term
Oftentimes when we are dealing with cases where we have two or more explanatory variables, we want to know if there is an interaction between them. What do we mean by interaction? An interaction is observed when the effect of two explanatory variables on the response variable is not additive (e.g., their effect could instead be synergistic). 

Our hypotheses for whether or not there is an interaction are:

**Null Hypothesis, $H_{0}$**: There is **no** interaction effect between sexual activity and thorax length on the mean longevity of the population.  
**Alternative Hypothesis, $H_{A}$**: There is an interaction effect between sexual activity and thorax length on the mean longevity of the population.

We can get an intuitive sense for this via visualization by making an interaction plot (see example below). In an interaction plot, we look at the slope of the lines that connect the group means. If the slopes of the lines are parallel, then there is no interaction between the explanatory variables. On the other hand, if they are not parallel, we can infer that there is an interaction effect.

Let's create the interaction plot for our case. Use the `fly_2_groups` data. Again, the response variable is `longevity` and the explanatory variables are `activity` (mapped to the x-axis) and `thorax` (mapped to color). Instead of boxplots, we will just plot points representing the mean of each category, and connect those means with lines.

```{r two-way_two_group_viz, exercise = TRUE}
<variable_name> %>%
    ggplot(aes(x = <first_explanatory_column>, y = <response_column>, color = <second_explanatory_column>)) +
  stat_summary(fun = mean,
               geom = "point",
               shape = 18,
               size = 4) +
  stat_summary(fun = mean,
               geom = "line",
               aes(group = thorax)) +
  labs(x = "Sexual Activity",
       y = "Longevity (days)")
```

```{r two-way_two_group_viz-solution, excercise.setup = "setup-two-group-sum"}
fly_2_groups %>%
  ggplot(aes(x = activity, y = longevity, color = thorax)) +
  stat_summary(fun = mean,
               geom = "point",
               shape = 18,
               size = 4) +
  stat_summary(fun = mean,
               geom = "line",
               aes(group = thorax)) +
  labs(x = "Sexual Activity",
       y = "Longevity (days)")
```

Although not perfectly parallel, the lines on the interaction plot are pretty close to parallel. So, the ANOVA results will very likely tell us that we will fail to reject the interaction effect null hypothesis. Let's proceed with the analysis to be sure.

One way to include an interaction term in your ANOVA model is to use the `*` symbol between two explanatory variables. This causes R to test the null hypotheses for the effect of each individual explanatory variable as well as the combined effect of these two explanatory variables. Thus, for us, our formula notation is now:

```
longevity ~ activity * thorax
```

Importantly, using `*` causes R to test all possible interactions. So if we had a formula `A * B * C`, it would test all three variables as well as all of their combinations. If instead, you want to identify specific interaction term(s), you can use `:`. In the case of our formula, this is the same as `*` but serves as an example:

```
longevity ~ activity + thorax + activity:thorax
```

Create a new model object called `fly_2_vars_model2` using the `fly_2_groups` data. Then use the `summary()` function to view the output of your ANOVA model. 

```{r two-way_two_group_aov, exercise = TRUE}
# create an ANOVA model object
<model_object> <- aov(<anova_formula>,
                    data = <data_frame>)

# view output of aov() 
<function_to_examine_model_output>(<model_object>)
```

```{r two-way_two_group_aov-solution, excercise.setup = "setup-two-group-sum"}
fly_2_vars_model2 <- aov(longevity ~ activity * thorax,
                          data = fly_2_groups)

summary(fly_2_vars_model2)
```

As a rule of thumb for cases such as these, we should always start by examining the output regarding the interaction effect (or lack thereof). We can see our output from ANOVA now has an additional line that refers interaction effect hypothesis, `activity:thorax`.

We observe that the p-value from this line is not less than the standard p-value threshold for rejecting null hypotheses (p > 0.05). Thus, as our interaction plot suggested, we fail to reject the null hypotheses and conclude that there is **no** interaction effect between sexual activity and thorax length on the mean longevity of the population).

We then proceed to investigate the hypotheses for each main effect independently, by interpreting the relevant p-values from our current ANOVA results table.

#### **Exercise 2: ANOVA**
![](images/rstudio_logo.png){width=0.4in} Exercise.
Determine whether the following statements are *true* or *false*

```{r aov_tf_1, echo=FALSE}
question("ANOVA tests the null hypothesis that the sample means are all equal?",
  answer("True"),
  answer("False", correct = TRUE),
  incorrect = "The null hypothesis says that the population (not sample) means are all equal."
)
```

```{r aov_tf_2, echo=FALSE}
question("We use ANOVA to compare the variances of the population?",
  answer("True"),
  answer("False", correct = TRUE),
  incorrect = "We use ANOVA to compare the population means for different groups"
)
```

```{r aov_tf_3, echo=FALSE}
question("A one-way ANOVA is equivalent to a t-test when there are 2 groups to be compared",
  answer("True", correct = TRUE),
  answer("False")
)
```

```{r aov_tf_4, echo=FALSE}
question("In rejecting the null hypothesis, one can conclude that all the population means are different from one another?",
  answer("True"),
  answer("False", correct = TRUE),
  incorrect = "One can conclude that at least one population mean is different from the others."
)
```

#### **Exercise 3: ANOVA cont.**
![](images/rstudio_logo.png){width=0.4in} Exercise.
Complete the following multiple choice questions. 

```{r aov_mc_1, echo=FALSE}
question("Analysis of variance is a statistical method comparing the _________ of several populations.",
  answer("standard deviations"),
  answer("proportions"),
  answer("means", correct = TRUE),
  answer("t-scores"),
  answer("none of the above"),
  allow_retry = TRUE
)
```

```{r aov_mc_2, echo=FALSE}
question("Which of the following is an assumption of one-way ANOVA comparing samples from three or more experimental treatments? Select all that apply.",
  answer("All the response variables within each population follow a normal distribution", correct = TRUE),
  answer("The sample sizes are approximately equal across groups"),
  answer("The samples associated with each population are randomly selected and are independent from all other samples", correct  = TRUE),
  answer("The response variables within each of the populations have equal variances", correct = TRUE),
  answer("The response variable is an interval or ratio variable"),
  answer("None of the above"),
  allow_retry = TRUE
)
```

## Linear regression
Now, we will work with a data frame that Jenny Bryan (UBC and RStudio) put together in the [`gapminder` package](https://www.gapminder.org/data/). 

Unlike the fruit fly data set, no pre-manipulation is needed so let's view the data as is. First, load the gapminder package and use the `head()` function to explore the data: 

```{r gapminder-package, exercise = TRUE}
# load and check the data
library(<package>)
<function>(gapminder)
```

```{r gapminder-package-solution}
library(gapminder)
head(gapminder)
```

We see that the data contain infomation on life expectancy (lifeExp), population (pop), and gross domestic product per capita (gdpPercap, a rough measure for economical richness) for many countries across many years. A very naive working hypothesis that you may come to is that our life expectancy grew with time. This would be represent in r with `lifeExp ~ year`. 

Let's explore this hypothesis graphically. Using the `gapminder` data set, create a scatterplot with `year` on the x-axis and `lifeExp` on the y-axis. Remember to create human readable labels!

```{r gapminder-plot, exercise = TRUE}
# uncomment the last line to customize 
<dataset> %>%
  ggplot(aes(x = <x-axis>, y = <y-axis>)) +
  geom_point() 
#labs(x = "x-axis", y = "y-axis")
```

```{r gapminder-plot-solution}
gapminder %>%
  ggplot(aes(x = year, y = lifeExp)) +
  geom_point() 
```

```{r setup-gapminder-plot, include = FALSE}
library(gapminder)
```

Although there is very high variance, we do see a certain trend with mean life expectancy increasing over time. Similarly, we can naively hypothesize that life expectancy is higher where the per-capita gdp is higher. In R, this is `lifeExp ~ gdpPercap`.

Again, let's explore this hypothesis graphically. Using the `gapminder` data set, create a scatterplot with `gdpPercap` on the x-axis and `lifeExp` on the y-axis. Have the x-axis be "Life expectancy (yrs)" and the y-axis label be "Per-capita GDP". 

```{r gapminder-plot2, exercise = TRUE}
# uncomment the last line to customize 
<dataset> %>%
  ggplot(aes(x = <x-axis>, y = <y-axis>)) +
  <geom> +
  labs(x = <x-axis label>, y = <y-axis label>)
```

```{r gapminder-plot2-solution}
gapminder %>%
  ggplot(aes(x = gdpPercap, y = lifeExp)) +
  geom_point() +
  labs(x = "Life expectancy (yrs)", y = "Per-capita GDP")
```

```{r setup-gapminder-plot2, include = FALSE}
library(gapminder)
```

### Linear models
A **linear regression** model describes the change of a *dependent variable*, say `lifeExp`, as a *linear* function of one or more *explanatory* variables, say *yaer*. This means tat increasing by $x$ the variable `year` will have an effect $\beta \cdot x$ on the *dependent* variable `lifeExp`, whatever the value $x$ is, in mathematical terms:
\[
\mbox{lifeExp} = \alpha + \beta \cdot \mbox{year}
\]

We call $\alpha$ the intercept of the model, or the value of `lifeExp` when `year` is equal to zero. When we go forward in time, increasing `year`, `lifeExp` increases (if $\beta$ is positive, otherwise it decreases):
\[
\alpha + \beta \cdot \left(\mbox{year} + x \right) = \alpha + \beta \cdot \mbox{year} + \beta \cdot x = \mbox{lifeExp} + \beta \cdot x
\]

### Key assumptions 
A number of assumptions must be satisfied for a linear model to be relaiable: These are: 

* the predictor variables should be measured with not too much error (**weak exogeneity**)
* the variance of the response variable should be roughly the same across the range of its predictors (**homoscedasticity**, a fancy pants word for "constant variance")
* the discrepancies between observed and predicted values should be **independent**
* the predictors themselves should be **non-colinear** (a rather technical issues, given by the way we solve the model, that may happen when two predictors are perfectly correlated or we try to estimate the effect of too many predictors with too little data).

Here, we only mention these assumptions, but for more details, take a look at [wiki](https://en.wikipedia.org/wiki/Linear_regression#Assumptions).

When we have only one predictive variable (what is called a _simple_ linear regression model), the formula we just introduced describes a straight line. The task of a linear regression method is identifying the _best fitting_ slope and intercept of that straight line. But what does _best fitting_ means in this context? We will first adopt a heuristic definition of it but will rigorously define it later on.

Let's consider a bunch of straight lines in our first plot:

```{r}
#Run this code chunk
gapminder %>%
  ggplot(aes(x = year, y = lifeExp)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0.033, colour = "green") +
  geom_abline(intercept = -575, slope = 0.32, colour = "purple") +
  geom_hline(aes(yintercept = mean(lifeExp)), colour = "red") +
  geom_vline(aes(xintercept = mean(year)), colour = "blue") +
  labs(y="Life expectancy (yrs)")
```

**So, which line best describes the data? To determine this, we must fit a linear model to the data.**

### Simple linear regression
To obtain the slope and intercept of the green line, we can use the built-in R function `lm()`. This function works very similarly to the `aov()` function we used earlier for our Anova model. 

Let's explore our earlier hypothesis that life expectancy is higher where the per-capita gdp is higher. Create a lm "model" object called `lifeExp_model1` using the `gapminder` data set. 

```{r gapminder-model1, exercise = TRUE}
<object_name> <- lm(lifeExp ~ year,
                    data = <data_set>)
```

```{r gapminder-model1-solution}
lifeExp_model1 <- lm(lifeExp ~ year,
                     data = gapminder)
```

```{r setup-gapminder-model1, include = FALSE}
library(gapminder)
```

Now, use the function `summary()` to see all the relevant results. 
```{r gapminder-model1-sum, exercise = TRUE}
summary(<object_name>)
```

```{r gapminder-model1-sum-solution}
summary(lifeExp_model1)
```

```{r setup-gapminder-model1-sum, include = FALSE}
lifeExp_model1 <- lm(lifeExp ~ year,
                     data = gapminder)
```

Now, however, we are more interested in more then just the p-values.

The `Estimate` values are the best foot for the intercept, $\alpha$, and the slope, $\beta$. The slope, the parameter that links `year` to `lifeExp`, is a positive value: every 1 year, the life expectancy increases of $`r summary(lifeExp_model1)$coefficients[2]`$ years. This is in line with our hypothesis. Moorever, its p-value, the probability of finding a correlation at least as strong between predictive and response variable, is rather low at $`r summary(lifeExp_model1)$coefficients[8]`$ (but see [this](https://backyardbrains.com/experiments/p-value) for a cautionary tale about p-values!).

Now, using the slope and intercept, we can plot the best fit line on our data. We can use the `geom_smooth()` function to do this. 

Here are the key arguments:

* `method` is the smoothing method to be used. Possible values include lm, glm, gam, loess, rlm. 
  * `lm` fits a linear model (this is the one we will be using for this example)
* `se` is a boolean value. If set to TRUE, the confidence interval will be displayed. 
* `color`, `size`, `linetype` changes the line color, size and type
* `fill` changes the fill color of the confidence region 

For this example, we will be adding a best fit line to our previous plot that looks at mean life expectancy increasing over time. Use the `geom_smooth()` function to create the line. Set method to `lm`, with no confidence intervals, and make the line green. 

```{r gapminder-line1, exercise = TRUE}
# play around with the key arguments!
gapminder %>%
  ggplot(aes(x = year, y = lifeExp)) +
  geom_point() +
  <add_geom>(method = <method>, se = <se>, color = <color>) 
```

```{r gapminder-line1-solution}
gapminder %>%
  ggplot(aes(x = year, y = lifeExp)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "green")
```

```{r setup-gapminder-line1, include = FALSE}
library(gapminder)
```

Another important bit of information in our results is the R-squared values, both are the _Multiple_ and _Adjusted R-squared_. These tell us how much of _variance_ in the life expectancy data is explained by the year. In this case, not much (`r summary(lifeExp_model1)$r.squared`, `r summary(lifeExp_model1)$adj.r.squared`, respectively)

### Residuals
We can further explore our linear model by plotting some diagnostic plots. Base R provides a quick and easy way to view all of these plots at once with `plot()`. 

```{r gapminder-res, exercise = TRUE}
# Set plot frame to 2 by 2
par(mfrow=c(2,2))
# Create diagnostic plots
plot(<model_object>)
```

```{r gapminder-res-solution}
par(mfrow=c(2,2))
plot(lifeExp_model1)
```

```{r setup-gapminder-res, include = FALSE}
lifeExp_model1 <- lm(lifeExp ~ year,
                     data = gapminder)
```

Whoa right!? Let's break it down. Overall, these diagnostic plots are useful for understanding the model *residuals*. The residuals are the discrepancies between the life expectancy we should have guessed by the model and the observed values in the available data. In other words, the distance between the straight line and actual data points. In a linear regression model, these residuals are the values we are trying to minimize when we fit a straight line. 

There is a lot of information contained in these 4 plots, and you can find in-depth explanations [here](https://data.library.virginia.edu/diagnostic-plots/). For our purposes today, let's focus on just the **Residuals vs Fitted** and **Normal Q-Q plots**.

The **Residuals vs Fitted** plot shows the differences between the best fit line and all the available data points. When the mmodel is a good fit for the data, this plot should have no discernable pattern. That is, the red line should *not* form a shape like an 'S' or a parabola. Another way to look at it is that the points should look like 'stars in the sky', *e.g.* random. This second description is not great for these data since year is an integer (whole number) but we do see that the red line is relatively straight and without pattern. 

The **Normal Q-Q plot** directly compares the best fit and actual data values. A good model closely adheres to the dotted line and points that fall off the line should not portray any pattern. In our case, this plot indicates that this simple linear model may not be the best fit for these data. Notice how either end deviates more and more from the line and the plot forms somewhat of an 'S' pattern.

These ends are particularly important in a linear model. Because we have chosen to use a simple linear model, *outliers* (observed values that are very far away from the best fit line), are very important. They have a high *leverage* (see the forth dignostic plot). This is especially true if the outliers are at the edge of the preddicting variable ranges such as we see in our Q-Q plot. 

#### **Exercise 1: Linear models**
![](images/rstudio_logo.png){width=0.4in} Exercise.
Determine whether the following statements are *true* or *false*

```{r lm_tf1, echo=FALSE}
question("In simple linear regression, residuals will be larger as observations are further from the mean of X.",
  answer("True"),
  answer("False", correct = TRUE),
  incorrect = "When observations are further from the mean of X, residuals get smaller."
)
```

```{r lm_tf2, echo=FALSE}
question("In simple linear regression, it is possible to obtain a slope estimate greater than 1.0.",
  answer("True", correct = TRUE),
  answer("False"),
  incorrect = "In simple linear regression, it is possible to obtain a slope estimate greater than 1.0."
)
```

#### **Exercise 2: Linear models cont.**
![](images/rstudio_logo.png){width=0.4in} Exercise.
Complete the following multiple choice questions. 

```{r lm_mmc1, echo=FALSE}
question("What does the Y intercept represent?",
  answer("The predicticed value of Y when x = 0", correct = TRUE),
  answer("The estimated change in average Y per unit change in X"),
  answer("The predicted value of Y"),
  answer("The variation around the line regression"),
  answer("none of the above"),
  allow_retry = TRUE
)
```

```{r lm_mc2, echo=FALSE}
question("What does the slope represent?",
  answer("The predicticed value of Y when x = 0"),
  answer("The estimated change in average Y per unit change in X", correct = TRUE),
  answer("The predicted value of Y"),
  answer("The variation around the line regression"),
  answer("None of the above"),
  allow_retry = TRUE
)
```

```{r lm_mc3, echo=FALSE}
question("Which one of the following is NOT appropriate for studying the relationship between two quantitative variables?",
  answer("Scattter plot"),
  answer("Bar plot", correct = TRUE),
  answer("Correlation"),
  answer("Regression"),
  answer("None of the above"),
  allow_retry = TRUE
)
```

#### **Exercise 3: Linear models cont.**
![](images/rstudio_logo.png){width=0.4in} Exercise.

Fit a linear model of life expectancy as a function of per-capita GDP from the gapminder data set. Use the plot you previously created to help you out.  

  * Create a model variable called `lm_exercise`
  * Use the `summary()` function to examine the output.
  * Use the `plot()` function to create the diagnostic plots
  
Do you think this is a good fit for these data?

**Note: Hints will be provided for exercises but no solution. Try to figure it out!**
```{r ex2, exercise = TRUE}
# Fit a linear model
<model_object> <- lm(<response_column> ~ <explanatory_column>,
                      data = <variable_name>)

# Check output of model 
<function_for_examining_output>(<model_object>)

# Set plot frame to 2 by 2
par(mfrow=c(2,2))
# Create diagnostic plots
<function_for_creating_diagnostic_plots>(<model_object>)
```

```{r ex2-hint-1}
# The response column is lifeExp and the exploratory column is gdpPercap
```

```{r ex2-hint-2}
# Name the model object lm_exercise
```

```{r ex2-hint-3}
# To examine the output of your model, use the summary() function!
```

```{r ex2-hint-4}
# To examine the diagnostic plots, use the plot() function!
```

## Cautions when using linear models
R (and most other statistical software) will fit, plot, summarize, etc a linear model regardless of whether that model is a good fit for the data. 

For example, the `Anscombe` data sets are very different data that give the *same* slope, intercept, and mean in a linear model. 

We can access these data in R and format them with:

```{r echo = TRUE, message = FALSE}
absc <- with(datasets::anscombe,
             tibble(X=c(x1,x2,x3,x4),
                        Y=c(y1,y2,y3,y4),
        anscombe_quartet=gl(4,nrow(datasets::anscombe))
                   )
             )
```

Plotting these data reveals the problem. 
```{r echo = TRUE, message = FALSE}
absc %>%
  ggplot(aes(x=X,y=Y,group=anscombe_quartet)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(limits = c(0,20)) +
  facet_wrap(~anscombe_quartet)
```

**This is why you should always, always, always plot your data before attempting regression!**

## Multiple linear regression
So far, we have dealt with simple regression models, where we had only one predictor value. However, `lm()` handles much more complex models. 
