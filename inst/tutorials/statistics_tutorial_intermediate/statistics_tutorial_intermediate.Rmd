---
title: "Intermediate Statistics Tutorial"
author: "Andrew Wilson"
date: "20/01/2020"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(learnr)
raw_dat <- read_csv("../../MICB405/R_module/Saanich_Data.csv")
library(DT)
```

## Overview
At its heart, R is statistical software. Thus, it should come as no surprise that base R allows you to perform many different statistical tests. Here, we cover:

* *t*-tests
* One-way ANOVA
* Simple linear models

## Setup 
The first step of any data science project is to examine the data you are working with. In our case, we are examining some measurements from the Saanich inlet dataset. 

```{r}
dim(raw_dat)
colnames(raw_dat)
```
### Data cleaning
We also want to remove entries with no oxygen measurements from our dataset.
```{r}
dat <- 
  raw_dat %>%
  select(Cruise, Date, Depth, Temperature,
         WS_O2, WS_NO3, WS_H2S) %>%
  filter(!is.na(WS_O2)) %>%
  rename(O2_uM=WS_O2, NO3_uM=WS_NO3, H2S_uM=WS_H2S) %>%
  mutate(Depth_m=Depth*1000)
```

## *t*-test
### Hypotheses

Null hypothesis H~0~: the two population means are equal  
Alternate hypothesis H~1~: the two population means are not equal

### Assumptions

1. Simple random sample
2. Approximately normal distributions of sample means
3. Reasonably large sample size
4. Equal sample sizes*
5. Equal sample variances*

$*$ Different equations exist if these are not true

### Subset data to 2 depths in 3 cruises in February.
```{r}
dat.ttest <- dat %>%
  filter(Depth_m %in% c(10, 100) & Cruise %in% c(18,30,42)) %>% 
  select(Cruise, Date, Depth_m, O2_uM)
```

### Visualize data
```{r}
dat.ttest %>% 
  
ggplot(aes(x=as.factor(Depth_m), y=O2_uM)) +
  geom_boxplot() +
  geom_point() +
  labs(x="Depth (m)", y="Oxygen (uM)")
```

### Fit a *t*-test and estimate p-values
```{r}
t.test(O2_uM ~ as.factor(Depth_m), data = dat.ttest, var.equal=FALSE)
```

## One-way ANOVA
Analysis of variance (ANOVA) allows us to compare more than the 2 groups possible in a *t*-test. It functions very similarly to the *t*-test, having the same assumptions, but tests fundamentally different hypotheses. We will investigate one-way ANOVAs, meaning there is a single explanatory variable.

### Hypotheses

Null hypothesis H~0~: *all* of the population means are equal to the mean across all populations  
Alternate hypothesis H~1~: *at least one* population mean is not equal to the mean across all populations

*Note that this does not test __which__ population mean is different from the overall mean.*

### Assumptions
(Same as *t*-test)

1. Simple random sample
2. Approximately normal distributions of sample means
3. Reasonably large sample size
4. Equal sample sizes
5. Equal sample variances


### Subset data to 3 depths in 3 cruises in February.
```{r}
dat.aov <- dat %>%
  filter(Depth_m %in% c(10, 100, 200) & Cruise %in% c(18,30,42)) %>% 
  select(Cruise, Date, Depth_m, O2_uM)
```

### Visualize the data
```{r}
dat.aov %>% 
  
ggplot(aes(x=as.factor(Depth_m), y=O2_uM)) +
  geom_boxplot() +
  geom_point() +
  geom_hline(aes(yintercept=mean(O2_uM))) + # Add a horizontal line for the overall mean
  labs(x="Depth (m)", y="Oxygen (uM)")
```

The horizontal line is the overall mean across all 3 depths. It is this mean that the sample means are being compared to.

### Fit ANOVA and estimate p-values
```{r}
summary(aov(O2_uM ~ as.factor(Depth_m), data = dat.aov))
```
Note that for the `aov` function, we need to summarize it to obtain p-values.

## Linear regression
Linear models take it another step further by allowing us to fit a model to 2 or more continuous variables. Here, we will go over the simplest regression `Y ~ X`.

### Hypotheses

Null hypothesis H~0~: The slope of the relationship between X and Y populations is zero.  
Alternate hypothesis H~1~: The slope of the relationship between X and Y populations is not zero.

### Assumptions

1. The relationship between X and Y is linear.
2. Residuals (the difference between the observed data and the linear fit):
    - are normally distributed
    - have a mean of approximately zero
    - have equal variance
    - are not auto-correlated
    - are not correlated with X

### Subset the data to 3 February cruises.
```{r}
dat.lm <- dat %>%
  filter(Cruise %in% c(18,30,42)) %>% 
  select(Cruise, Date, Depth_m, O2_uM)
```

### Visualize the data
```{r}
dat.lm %>% 
  
ggplot(aes(x=Depth_m, y=O2_uM)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x="Depth (m)", y="Oxygen (uM)")
```

Note how ggplot can add the linear fit (blue line) and 95% confidence intervals (grey area) with `geom_smooth`.

### Fit the linear model and estimate p-values
```{r}
summary(lm(O2_uM ~ Depth_m, data = dat.lm))
```

## Testing assumptions
### *t*-test and ANOVA
In general, you can determine if your data adhere to the assumptions of a *t*-test or ANOVA simply by understanding where the data came from and completely a few exploratory tests like below.

#### Are the data normally distributed?
```{r}
ggplot(dat.aov, aes(x=O2_uM)) +
  geom_histogram(bins=6)
```

#### Are the sample sizes large and equal? Are the sample variances equal?
```{r}
dat.aov %>% 
  group_by(Depth_m) %>% 
  summarise(n = n(), 
            sd = sd(O2_uM), 
            variance = var(O2_uM))
```

And we see that these data break pretty much all the assumptions. Fortunately, these were merely a subset of the full data and we know that depth is, in fact, linear so a linear model would be a better fit.

### Linear regression
But the assumptions of a linear model mostly revolve around residuals. Thus, simple exploration of the raw data won't help. Instead, we can view the residuals using plots like so.

```{r}
# Set window to 2x2 plots
par(mfrow=c(2,2))
# Use base R to plot
plot(lm(O2_uM ~ Depth_m, data = dat.lm))
```

What you want to see is flat (horizontal) red fit lines and randomly distributed points (plots 1, 3, 4) as well as points mostly along the dotted fit line (plot 2). 

You can also quickly test the most pertinent assumptions using the package `gvlma`. *Remember, you will need to install this package before you can load it.*

```{r}
library(gvlma)
gvlma(lm(O2_uM ~ Depth_m, data = dat.lm))
```

From these results, we can conclude that a linear model is an okay fit for oxygen by depth but likely not the best fit as there is a clear sigmodial shape in  the residual vs. fitted plot. You can also see this clearly if you allow ggplot to fit a polynomial model with `loess`.
```{r}
dat.lm %>% 
  
ggplot(aes(x=Depth_m, y=O2_uM)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x="Depth (m)", y="Oxygen (uM)")
```

Thus, we would next test if a sigmodial model was a better fit. However, this is outside the scope of this module.

## Breaking assumptions
So what do you do if your data is non-normal? Or has small sample sizes? Or breaks any of the given assumptions of a statistical test?

The first thing is that you *should not* be using that test! There are many, many, many statistical tests for all types of data including non-normal, uneven, and/or small data sets. These, however, are also beyond the scope of this module.

When in doubt, consult a statistician! Science is increasingly collaborative and interdisciplinary so take advantage of other expertise!

## Additional resources

* [R cheatsheets](https://www.rstudio.com/resources/cheatsheets/) also available in RStudio under Help > Cheatsheets

* [Free online statistics book](http://onlinestatbook.com/2/regression/intro.html)
* Applied Statistics and Data Science Group ([ASDa](https://asda.stat.ubc.ca/)) at UBC
